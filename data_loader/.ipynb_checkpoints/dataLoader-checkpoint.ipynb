{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd93073c-3c2f-4a2a-8cd9-690d68d321ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data storage and image processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.preprocessing import image\n",
    "# TODO: figure out layers and method of training\n",
    "\n",
    "# SKLearn Libraries\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c0ffd-3ac3-4675-859b-5ebfc7c22522",
   "metadata": {},
   "source": [
    "Now we read in the data from the local directory where it is stored, and process the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8e4b7ea-2b7c-4fde-ac95-3f3f7addf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define the paths to our files\n",
    "# Current directory structure: train has subdirectories A-Z, each with images stored inside,\n",
    "# test just has the files with the letter, label is in file name.\n",
    "train_images = '../dataset/asl_alphabet_train/asl_alphabet_train'\n",
    "test_images = '../dataset/asl_alphabet_test/asl_alphabet_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f1a482-6fa1-46d6-a975-6c000b137801",
   "metadata": {},
   "source": [
    "# Load Data #\n",
    "\n",
    "Read in the data from the local directory and store it in a pandas DataFrame. We will display the first 5 rows with df.head() to validate our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f297bfb1-d122-4883-9cb2-56db07eda3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2990</th>\n",
       "      <th>2991</th>\n",
       "      <th>2992</th>\n",
       "      <th>2993</th>\n",
       "      <th>2994</th>\n",
       "      <th>2995</th>\n",
       "      <th>2996</th>\n",
       "      <th>2997</th>\n",
       "      <th>2998</th>\n",
       "      <th>2999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>A1.jpg</td>\n",
       "      <td>A10.jpg</td>\n",
       "      <td>A100.jpg</td>\n",
       "      <td>A1000.jpg</td>\n",
       "      <td>A1001.jpg</td>\n",
       "      <td>A1002.jpg</td>\n",
       "      <td>A1003.jpg</td>\n",
       "      <td>A1004.jpg</td>\n",
       "      <td>A1005.jpg</td>\n",
       "      <td>A1006.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>A990.jpg</td>\n",
       "      <td>A991.jpg</td>\n",
       "      <td>A992.jpg</td>\n",
       "      <td>A993.jpg</td>\n",
       "      <td>A994.jpg</td>\n",
       "      <td>A995.jpg</td>\n",
       "      <td>A996.jpg</td>\n",
       "      <td>A997.jpg</td>\n",
       "      <td>A998.jpg</td>\n",
       "      <td>A999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>B1.jpg</td>\n",
       "      <td>B10.jpg</td>\n",
       "      <td>B100.jpg</td>\n",
       "      <td>B1000.jpg</td>\n",
       "      <td>B1001.jpg</td>\n",
       "      <td>B1002.jpg</td>\n",
       "      <td>B1003.jpg</td>\n",
       "      <td>B1004.jpg</td>\n",
       "      <td>B1005.jpg</td>\n",
       "      <td>B1006.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>B990.jpg</td>\n",
       "      <td>B991.jpg</td>\n",
       "      <td>B992.jpg</td>\n",
       "      <td>B993.jpg</td>\n",
       "      <td>B994.jpg</td>\n",
       "      <td>B995.jpg</td>\n",
       "      <td>B996.jpg</td>\n",
       "      <td>B997.jpg</td>\n",
       "      <td>B998.jpg</td>\n",
       "      <td>B999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>C1.jpg</td>\n",
       "      <td>C10.jpg</td>\n",
       "      <td>C100.jpg</td>\n",
       "      <td>C1000.jpg</td>\n",
       "      <td>C1001.jpg</td>\n",
       "      <td>C1002.jpg</td>\n",
       "      <td>C1003.jpg</td>\n",
       "      <td>C1004.jpg</td>\n",
       "      <td>C1005.jpg</td>\n",
       "      <td>C1006.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>C990.jpg</td>\n",
       "      <td>C991.jpg</td>\n",
       "      <td>C992.jpg</td>\n",
       "      <td>C993.jpg</td>\n",
       "      <td>C994.jpg</td>\n",
       "      <td>C995.jpg</td>\n",
       "      <td>C996.jpg</td>\n",
       "      <td>C997.jpg</td>\n",
       "      <td>C998.jpg</td>\n",
       "      <td>C999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>D1.jpg</td>\n",
       "      <td>D10.jpg</td>\n",
       "      <td>D100.jpg</td>\n",
       "      <td>D1000.jpg</td>\n",
       "      <td>D1001.jpg</td>\n",
       "      <td>D1002.jpg</td>\n",
       "      <td>D1003.jpg</td>\n",
       "      <td>D1004.jpg</td>\n",
       "      <td>D1005.jpg</td>\n",
       "      <td>D1006.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>D990.jpg</td>\n",
       "      <td>D991.jpg</td>\n",
       "      <td>D992.jpg</td>\n",
       "      <td>D993.jpg</td>\n",
       "      <td>D994.jpg</td>\n",
       "      <td>D995.jpg</td>\n",
       "      <td>D996.jpg</td>\n",
       "      <td>D997.jpg</td>\n",
       "      <td>D998.jpg</td>\n",
       "      <td>D999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>del</th>\n",
       "      <td>del1.jpg</td>\n",
       "      <td>del10.jpg</td>\n",
       "      <td>del100.jpg</td>\n",
       "      <td>del1000.jpg</td>\n",
       "      <td>del1001.jpg</td>\n",
       "      <td>del1002.jpg</td>\n",
       "      <td>del1003.jpg</td>\n",
       "      <td>del1004.jpg</td>\n",
       "      <td>del1005.jpg</td>\n",
       "      <td>del1006.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>del990.jpg</td>\n",
       "      <td>del991.jpg</td>\n",
       "      <td>del992.jpg</td>\n",
       "      <td>del993.jpg</td>\n",
       "      <td>del994.jpg</td>\n",
       "      <td>del995.jpg</td>\n",
       "      <td>del996.jpg</td>\n",
       "      <td>del997.jpg</td>\n",
       "      <td>del998.jpg</td>\n",
       "      <td>del999.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1           2            3            4            5     \\\n",
       "A      A1.jpg    A10.jpg    A100.jpg    A1000.jpg    A1001.jpg    A1002.jpg   \n",
       "B      B1.jpg    B10.jpg    B100.jpg    B1000.jpg    B1001.jpg    B1002.jpg   \n",
       "C      C1.jpg    C10.jpg    C100.jpg    C1000.jpg    C1001.jpg    C1002.jpg   \n",
       "D      D1.jpg    D10.jpg    D100.jpg    D1000.jpg    D1001.jpg    D1002.jpg   \n",
       "del  del1.jpg  del10.jpg  del100.jpg  del1000.jpg  del1001.jpg  del1002.jpg   \n",
       "\n",
       "            6            7            8            9     ...        2990  \\\n",
       "A      A1003.jpg    A1004.jpg    A1005.jpg    A1006.jpg  ...    A990.jpg   \n",
       "B      B1003.jpg    B1004.jpg    B1005.jpg    B1006.jpg  ...    B990.jpg   \n",
       "C      C1003.jpg    C1004.jpg    C1005.jpg    C1006.jpg  ...    C990.jpg   \n",
       "D      D1003.jpg    D1004.jpg    D1005.jpg    D1006.jpg  ...    D990.jpg   \n",
       "del  del1003.jpg  del1004.jpg  del1005.jpg  del1006.jpg  ...  del990.jpg   \n",
       "\n",
       "           2991        2992        2993        2994        2995        2996  \\\n",
       "A      A991.jpg    A992.jpg    A993.jpg    A994.jpg    A995.jpg    A996.jpg   \n",
       "B      B991.jpg    B992.jpg    B993.jpg    B994.jpg    B995.jpg    B996.jpg   \n",
       "C      C991.jpg    C992.jpg    C993.jpg    C994.jpg    C995.jpg    C996.jpg   \n",
       "D      D991.jpg    D992.jpg    D993.jpg    D994.jpg    D995.jpg    D996.jpg   \n",
       "del  del991.jpg  del992.jpg  del993.jpg  del994.jpg  del995.jpg  del996.jpg   \n",
       "\n",
       "           2997        2998        2999  \n",
       "A      A997.jpg    A998.jpg    A999.jpg  \n",
       "B      B997.jpg    B998.jpg    B999.jpg  \n",
       "C      C997.jpg    C998.jpg    C999.jpg  \n",
       "D      D997.jpg    D998.jpg    D999.jpg  \n",
       "del  del997.jpg  del998.jpg  del999.jpg  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "categories = []\n",
    "data = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(train_images, topdown=True):\n",
    "    directory_name = os.path.basename(subdir).split(\"\\\\\")[0]\n",
    "    # Don't include the directory we are currently in, trying to get letter categories.\n",
    "    if(directory_name != 'asl_alphabet_train'):\n",
    "        categories.append(os.path.basename(subdir).split(\"\\\\\")[0])\n",
    "    directory_data = []\n",
    "    for file in files:\n",
    "        directory_data.append(file)\n",
    "\n",
    "    data.append(directory_data)\n",
    "\n",
    "data = [directory for directory in data if directory] # Remove null elements\n",
    "df = pd.DataFrame(data, categories)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413320a-0786-4e74-be46-99ccd67e58f0",
   "metadata": {},
   "source": [
    "Some information about our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bedd1e86-5d7f-4c2f-9180-d9c0e0177c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (29, 3000)\n",
      "Dataset size: 87000\n",
      "Data type: 0       object\n",
      "1       object\n",
      "2       object\n",
      "3       object\n",
      "4       object\n",
      "         ...  \n",
      "2995    object\n",
      "2996    object\n",
      "2997    object\n",
      "2998    object\n",
      "2999    object\n",
      "Length: 3000, dtype: object\n",
      "Dataset columns: RangeIndex(start=0, stop=3000, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset size: {df.size}\")\n",
    "print(f\"Data type: {df.dtypes}\")\n",
    "print(f\"Dataset columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c91436-7cfc-48c3-a0fd-50b355f54966",
   "metadata": {},
   "source": [
    "#### Preprocess data if necessary (i.e. drop certain columns we are not using). Since pandas DataFrames do not support images, we will read the data into a numpy array. Since the dataset is considerably large, we will use a Keras function to support batching ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4eff74ae-6bc7-43eb-badd-fb98c8c4ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 files belonging to 29 classes.\n",
      "Using 78300 files for training.\n",
      "Using 8700 files for validation.\n"
     ]
    }
   ],
   "source": [
    "'''train_images_list = []\n",
    "for index, row in df.iterrows():\n",
    "    train_images_dir = f\"{train_images}/{index}\"\n",
    "    check = True\n",
    "    for value in row:\n",
    "        image_path = f\"{train_images_dir}/{value}\"\n",
    "        img = image.load_img(image_path, target_size=(200, 200))\n",
    "        img = image.img_to_array(img)\n",
    "        train_images_list.append(img)\n",
    "\n",
    "training_images = np.array(train_images_list)'''\n",
    "directory = train_images\n",
    "training_images, validation_images = keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='both', # we will use 10% data for validation\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94c46a10-393f-4f74-bf7d-ab29cba99c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(training_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88419f74-9424-4728-8ac4-c4c71b064425",
   "metadata": {},
   "source": [
    "### Define the model: we will use a sequential model, with a relu activation function and MaxPooling 2D layers, to extract the image features, then  ###\n",
    "\n",
    "Conv2D documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
    "    We will use the default kernel initializer and no bias for now, but can change later for optimization.\n",
    "MaxPooling2D documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fecd969f-c242-451a-a914-bc27ac5c0a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">921600</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │     <span style=\"color: #00af00; text-decoration-color: #00af00\">235,929,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,453</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │           \u001b[38;5;34m7,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_15 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m590,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_16 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m590,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m921600\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │     \u001b[38;5;34m235,929,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                  │           \u001b[38;5;34m7,453\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">237,124,637</span> (904.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m237,124,637\u001b[0m (904.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">237,124,637</span> (904.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m237,124,637\u001b[0m (904.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Input(shape=(256, 256, 3)))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "# For pooling layer, downscale by a factor of 2\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "\n",
    "# Perform classification by feeding final output tensor into dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "\n",
    "# Using softmax because of this article, could change later: https://emeritus.org/blog/cnn-neural-network/#:~:text=The%20Fully%20Connected%20Layer:%20Making,applications%20such%20as%20image%20recognition.\n",
    "model.add(layers.Dense(29, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50effcd6-a36c-4db6-a880-710a844c0ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m   1/2447\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47:12:46\u001b[0m 69s/step - accuracy: 0.0312 - loss: 14.9456"
     ]
    }
   ],
   "source": [
    "# We will use the Adam optimizer, for the performance reasons discussed in the following link: https://www.geeksforgeeks.org/adam-optimizer/\n",
    "# Can potentially use other loss functions later for performance boost, this one chosen because\n",
    "# of how the labels are meant to be classified (one-hot encoded 1-29)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 1 # Can modify later\n",
    "# Fit the model to training data\n",
    "model.fit(x=training_images, validation_data=validation_images, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7d910-b48f-4107-8f5e-648b710fe1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
