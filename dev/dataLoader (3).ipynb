{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd93073c-3c2f-4a2a-8cd9-690d68d321ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd93073c-3c2f-4a2a-8cd9-690d68d321ad",
    "outputId": "652824b8-2640-44f2-bb20-de5031a25359"
   },
   "outputs": [],
   "source": [
    "# Libraries for data storage and image processing\n",
    "'''!python --version\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install pillow\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install opencv-python '''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "from keras import metrics, models, layers\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "\n",
    "# SKLearn Libraries\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c0ffd-3ac3-4675-859b-5ebfc7c22522",
   "metadata": {
    "id": "888c0ffd-3ac3-4675-859b-5ebfc7c22522"
   },
   "source": [
    "Now we read in the data from the local directory where it is stored, and process the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e4b7ea-2b7c-4fde-ac95-3f3f7addf209",
   "metadata": {
    "id": "a8e4b7ea-2b7c-4fde-ac95-3f3f7addf209"
   },
   "outputs": [],
   "source": [
    "# We will define the paths to our files, each of these sets are for a different training environment\n",
    "# Current directory structure: train has subdirectories A-Z, each with images stored inside,\n",
    "# test just has the files with the letter, label is in file name.\n",
    "\n",
    "# train_images = '../dataset/asl_alphabet_train/asl_alphabet_train'\n",
    "# test_images = '../dataset/asl_alphabet_test/asl_alphabet_test'\n",
    "\n",
    "# train_images = '/content/drive/My Drive/dataset/asl_alphabet_train/asl_alphabet_train'\n",
    "# test_images = '/content/drive/My Drive/dataset/asl_alphabet_test/asl_alphabet_test'\n",
    "\n",
    "train_images = '/lustre/isaac/scratch/jdosch1/dataset_COSC307/asl_alphabet_train/asl_alphabet_train'\n",
    "test_images = '/lustre/isaac/scratch/jdosch1/dataset_COSC307/asl_alphabet_test/asl_alphabet_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4003b8e-85e9-47ec-a7ee-25cb7bd2ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f1a482-6fa1-46d6-a975-6c000b137801",
   "metadata": {
    "id": "55f1a482-6fa1-46d6-a975-6c000b137801"
   },
   "source": [
    "# Load Data #\n",
    "\n",
    "Read in the data from the local directory and store it in a pandas DataFrame. We will display the first 5 rows with df.head() to validate our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f297bfb1-d122-4883-9cb2-56db07eda3f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f297bfb1-d122-4883-9cb2-56db07eda3f7",
    "outputId": "698ce68c-54c0-47ea-d9b4-9a8f86b97e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "categories = []\n",
    "data = []\n",
    "exclude_columns = {'del', 'asl_alphabet_train', 'nothing', 'space', 'J', 'Z'}\n",
    "\n",
    "# For dataset restriction\n",
    "include_columns = ['O', 'W', 'C', 'L', 'Y']\n",
    "\n",
    "for subdir, dirs, files in os.walk(train_images, topdown=True):\n",
    "    directory_name = os.path.basename(subdir)\n",
    "    # Don't include the directory we are currently in, trying to get letter categories.\n",
    "    if(directory_name in include_columns):\n",
    "        categories.append(directory_name)\n",
    "        directory_data = []\n",
    "        for file in files:\n",
    "          directory_data.append(file)\n",
    "\n",
    "        data.append(directory_data)\n",
    "\n",
    "sorted_categories_indices = sorted(range(len(categories)), key=lambda i: categories[i])\n",
    "categories = [categories[i] for i in sorted_categories_indices]\n",
    "data = [data[i] for i in sorted_categories_indices]\n",
    "\n",
    "data = [directory for directory in data if directory] # Remove null elements\n",
    "df = pd.DataFrame(data, categories)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413320a-0786-4e74-be46-99ccd67e58f0",
   "metadata": {
    "id": "0413320a-0786-4e74-be46-99ccd67e58f0"
   },
   "source": [
    "Some information about our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bedd1e86-5d7f-4c2f-9180-d9c0e0177c58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bedd1e86-5d7f-4c2f-9180-d9c0e0177c58",
    "outputId": "2c465f21-ef6a-4c6a-8a85-f21202dab92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (0, 0)\n",
      "Dataset size: 0\n",
      "Data type: Series([], dtype: object)\n",
      "Dataset columns: RangeIndex(start=0, stop=0, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset size: {df.size}\")\n",
    "print(f\"Data type: {df.dtypes}\")\n",
    "print(f\"Dataset columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c91436-7cfc-48c3-a0fd-50b355f54966",
   "metadata": {
    "id": "79c91436-7cfc-48c3-a0fd-50b355f54966"
   },
   "source": [
    "#### Preprocess data if necessary (i.e. drop certain columns we are not using). Since pandas DataFrames do not support images, we will read the data into a numpy array. Since the dataset is considerably large, we will use a Keras function to support batching ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eff74ae-6bc7-43eb-badd-fb98c8c4ac35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "4eff74ae-6bc7-43eb-badd-fb98c8c4ac35",
    "outputId": "f6b8f5e5-d2d5-48c5-f8e8-c9843f6c9b96"
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory /lustre/isaac/scratch/jdosch1/dataset_COSC307/asl_alphabet_train/asl_alphabet_train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read dataset into a dataloader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m train_images\n\u001b[1;32m----> 3\u001b[0m training_images, validation_images \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m      4\u001b[0m     directory,\n\u001b[0;32m      5\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     label_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     class_names\u001b[38;5;241m=\u001b[39minclude_columns,\n\u001b[0;32m      8\u001b[0m     color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     10\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[0;32m     11\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     12\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     14\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# we will use 20% data for validation\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m class_names \u001b[38;5;241m=\u001b[39m training_images\u001b[38;5;241m.\u001b[39mclass_names\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass names: \u001b[39m\u001b[38;5;124m\"\u001b[39m, class_names)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COSC307FinalProject\\Lib\\site-packages\\keras\\src\\utils\\image_dataset_utils.py:224\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 224\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mindex_directory(\n\u001b[0;32m    225\u001b[0m     directory,\n\u001b[0;32m    226\u001b[0m     labels,\n\u001b[0;32m    227\u001b[0m     formats\u001b[38;5;241m=\u001b[39mALLOWLIST_FORMATS,\n\u001b[0;32m    228\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mclass_names,\n\u001b[0;32m    229\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    230\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    231\u001b[0m     follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[0;32m    232\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    233\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COSC307FinalProject\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:530\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COSC307FinalProject\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    778\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory /lustre/isaac/scratch/jdosch1/dataset_COSC307/asl_alphabet_train/asl_alphabet_train"
     ]
    }
   ],
   "source": [
    "# Read dataset into a dataloader\n",
    "directory = train_images\n",
    "training_images, validation_images = keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=include_columns,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=128,\n",
    "    image_size=(128, 128),\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='both', # we will use 20% data for validation\n",
    ")\n",
    "\n",
    "class_names = training_images.class_names\n",
    "print(\"Class names: \", class_names)\n",
    "\n",
    "# Progress bar for augmentation\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Standardize dataset\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for images, _ in tqdm(training_images.unbatch()):\n",
    "    images = images.numpy()\n",
    "    images = images.flatten()\n",
    "    means.append(images.mean())\n",
    "    stds.append(images.std())\n",
    "\n",
    "\n",
    "means = np.mean(means)\n",
    "stds = np.std(stds)\n",
    "print(\"Mean: \", means)\n",
    "print(\"std: \", stds)\n",
    "\n",
    "# Check current datatype\n",
    "for images, _ in training_images.take(1): \n",
    "    print(type(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833b5ca-9aba-419c-82fc-968cddb119a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_images)\n",
    "\n",
    "brightness_layer = tf.keras.layers.RandomBrightness(factor=(0.0, 0.3)) # factor is the range for adjustment\n",
    "contrast_layer = tf.keras.layers.RandomContrast(factor=(0.6)) # factor determines contrast adjustment range\n",
    "gaussian_noise_layer = tf.keras.layers.GaussianNoise(stddev=0.35)\n",
    "rotation_layer = tf.keras.layers.RandomRotation(factor=(-1/36, 1/36), fill_mode='reflect')\n",
    "translation_layer = tf.keras.layers.RandomTranslation(height_factor=(-0.20, 0.20), width_factor=(-0.20, 0.20), fill_mode='reflect')\n",
    "zoom_layer = tf.keras.layers.RandomZoom(height_factor=(-0.10, 0.10), width_factor=(-0.10, 0.10), fill_mode='reflect', interpolation='bilinear')\n",
    "\n",
    "# Augment the data to make the model more generalized by adding:\n",
    "# brightness, contrast, blur, rotation, translation, and zoom at random\n",
    "augmentation = tf.keras.Sequential([\n",
    "    brightness_layer,\n",
    "    contrast_layer,\n",
    "    gaussian_noise_layer,\n",
    "    rotation_layer,\n",
    "    translation_layer, \n",
    "    zoom_layer\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf0586-3486-41de-991d-611f322d78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_process(image, label):\n",
    "    image = augmentation(image)\n",
    "    return image, label\n",
    "\n",
    "training_images_aug = training_images.map(lambda x, y: (augmentation(x, training=True), y))\n",
    "    \n",
    "# Standardization\n",
    "def standardize(image, label):\n",
    "    image = (image - means) / stds\n",
    "    return image, label\n",
    "\n",
    "training_images_aug = training_images_aug.map(standardize)\n",
    "validation_images = validation_images.map(standardize)\n",
    "\n",
    "# Check current datatype\n",
    "for images, _ in training_images_aug.take(1): \n",
    "    print(type(images[0]))\n",
    "    \n",
    "for images, _ in training_images_aug.take(1):  # Take a single batch\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(images)):\n",
    "        # Display augmented images\n",
    "        ax = plt.subplot(16, 16, i * 2 + 1)\n",
    "        plt.imshow(images[i].numpy(), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88419f74-9424-4728-8ac4-c4c71b064425",
   "metadata": {
    "id": "88419f74-9424-4728-8ac4-c4c71b064425"
   },
   "source": [
    "### Define the model: we will use a sequential model, with a relu activation function and MaxPooling 2D layers, to extract the image features, then  ###\n",
    "\n",
    "Conv2D documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
    "    We will use the default kernel initializer and no bias for now, but can change later for optimization.\n",
    "MaxPooling2D documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd969f-c242-451a-a914-bc27ac5c0a40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "fecd969f-c242-451a-a914-bc27ac5c0a40",
    "outputId": "27145584-01fb-45b0-aed3-44f9fc034ae4"
   },
   "outputs": [],
   "source": [
    "# Mimicking the VGG16 architecture with grayscale inputs\n",
    "model = models.Sequential()\n",
    "model.add(Input(shape=(128, 128, 1)))\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "'''\n",
    "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "'''\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "#model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Using softmax because of this article, could change later: https://emeritus.org/blog/cnn-neural-network/#:~:text=The%20Fully%20Connected%20Layer:%20Making,applications%20such%20as%20image%20recognition.\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50effcd6-a36c-4db6-a880-710a844c0ff3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50effcd6-a36c-4db6-a880-710a844c0ff3",
    "outputId": "56d964d2-cffe-49fb-d713-56d5c43c4df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10387s\u001b[0m 23s/step - accuracy: 0.0551 - loss: 3.4578 - val_accuracy: 0.2515 - val_loss: 2.9044 - learning_rate: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10399s\u001b[0m 23s/step - accuracy: 0.1728 - loss: 2.8281 - val_accuracy: 0.5978 - val_loss: 1.7604 - learning_rate: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10322s\u001b[0m 23s/step - accuracy: 0.4218 - loss: 1.8916 - val_accuracy: 0.8060 - val_loss: 0.8605 - learning_rate: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10344s\u001b[0m 23s/step - accuracy: 0.6265 - loss: 1.1770 - val_accuracy: 0.8827 - val_loss: 0.4904 - learning_rate: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10364s\u001b[0m 23s/step - accuracy: 0.7424 - loss: 0.8025 - val_accuracy: 0.9290 - val_loss: 0.3062 - learning_rate: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10321s\u001b[0m 23s/step - accuracy: 0.8077 - loss: 0.5844 - val_accuracy: 0.9530 - val_loss: 0.2045 - learning_rate: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10352s\u001b[0m 23s/step - accuracy: 0.8554 - loss: 0.4398 - val_accuracy: 0.9648 - val_loss: 0.1499 - learning_rate: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - accuracy: 0.8851 - loss: 0.3490 "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min') # could use restore_best_weights, reference: https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "# SGD chosen to adapt learning rate to break through plateaus\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=SGD(learning_rate=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Set save model weights callback in case we need to stop and restart training\n",
    "save_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   filepath = \"./VGGweights.11_27.weights.h5\", verbose=1, save_weights_only=True,\n",
    "   save_freq='epoch')\n",
    "\n",
    "# Set learning rate scheduler to prevent stagnation during training\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Set custom early stop function\n",
    "class MyThresholdCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(MyThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        val_acc = logs[\"val_accuracy\"]\n",
    "        if val_acc >= self.threshold:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Reference for custom callback: https://stackoverflow.com/questions/59563085/how-to-stop-training-when-it-hits-a-specific-validation-accuracy\n",
    "custom_callback = MyThresholdCallback(threshold=0.97)\n",
    "EPOCHS = 1000 # Can modify later\n",
    "\n",
    "# Fit the model to training data\n",
    "history = model.fit(x=training_images_aug, validation_data=validation_images, epochs=EPOCHS, batch_size = 128, callbacks=[custom_callback, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7d910-b48f-4107-8f5e-648b710fe1e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "89d7d910-b48f-4107-8f5e-648b710fe1e4",
    "outputId": "16d4df15-0351-41a1-d29a-8ea9ebb75c37"
   },
   "outputs": [],
   "source": [
    "# Plot the val loss and train loss to determine if the model is overfitting\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GOREbzlW2XdY",
   "metadata": {
    "id": "GOREbzlW2XdY"
   },
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "model.save('./COSC307_canny_edge.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
